#!/usr/bin/env python3
"""
Demo script for aitllm streaming inference. 
"""\nimport argparse\nimport torch\nfrom aitllm import StreamingEngine\n\ndef main():\n    parser = argparse.ArgumentParser(description="aitllm inference demo")\n    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-7B-Instruct",\n                        help="HuggingFace model ID or local path")\n    parser.add_argument("--prompt", type=str, default="Explain the theory of relativity in simple terms.",\n                        help="Input prompt")\n    parser.add_argument("--max_new_tokens", type=int, default=128,\n                        help="Maximum tokens to generate")\n    parser.add_argument("--device", type=str, default="cuda:0",\n                        help="Device to use")\n    parser.add_argument("--temperature", type=float, default=0.7,\n                        help="Sampling temperature")\n    parser.add_argument("--top_p", type=float, default=0.9,\n                        help="Nucleus sampling threshold")\n    \n    args = parser.parse_args()\n    \n    print(f"[aitllm] Initializing streaming engine...")\n    print(f"  Model: {args.model}")\n    print(f"  Device: {args.device}")\n    print(f"  CUDA Available: {torch.cuda.is_available()}")\n    \n    if torch.cuda.is_available():\n        print(f"  GPU: {torch.cuda.get_device_name(0)}")\n        print(f"  VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")\n    \n    engine = StreamingEngine(\n        model_path=args.model,\n        max_length=512,\n        device=args.device,\n        temperature=args.temperature,\n        top_p=args.top_p\n    )\n    \n    print(f"\n[aitllm] Prompt: {args.prompt}")\n    print(f"[aitllm] Generating...\n")\n    print("-" * 80)\n    \n    output = engine.generate(\n        prompt=args.prompt,\n        max_new_tokens=args.max_new_tokens,\n        stream=True\n    )\n    \n    print(output)\n    print("-" * 80)\n    \n    # Print statistics\n    stats = engine.get_stats()\n    print(f"\n[aitllm] Statistics:")\n    print(f"  Tokens Generated: {stats['tokens_generated']}")\n    print(f"  Time Elapsed: {stats['time_elapsed']:.2f}s")\n    print(f"  Tokens/sec: {stats['tokens_per_second']:.2f}")\n    print(f"  Peak VRAM: {stats['peak_vram_gb']:.2f} GB")\n    print(f"  Avg VRAM: {stats['avg_vram_gb']:.2f} GB")\n\n\nif __name__ == '__main__': \n    main()